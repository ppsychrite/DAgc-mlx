{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b6f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re \n",
    "from glob import glob\n",
    "from mlx_embeddings import load, generate \n",
    "from functools import reduce\n",
    "import mlx.core as mx\n",
    "import mlx.nn as nn \n",
    "import mlx.optimizers as optim \n",
    "import numpy as np \n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "data_dir = \"MRDA-Corpus/mrda_data\"\n",
    "model_name = \"answerdotai/ModernBERT-base\"\n",
    "\n",
    "model, tokenizer = load(model_name)\n",
    "\n",
    "def get_dialog(file_paths: list[str]) -> dict[str, list]:\n",
    "    cache = {} \n",
    "\n",
    "    for file_path in file_paths: \n",
    "        file = open(file_path, 'r')\n",
    "\n",
    "        lines = file.readlines()\n",
    "\n",
    "        tuples = [] \n",
    "        for l in lines: \n",
    "            ls = l.strip().split('|')\n",
    "\n",
    "            author = ls[0]\n",
    "            text = ls[1]\n",
    "            da = ls[-1]\n",
    "\n",
    "            tokens = tokenizer.tokenize(text)\n",
    "            tokens = [tokenizer.cls_token_id] + tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "            tuples.append({\n",
    "                'speaker' : author, \n",
    "                'dialog_act' : da,\n",
    "                'input_ids' : tokens, \n",
    "                'input_mask' : [1] * len(tokens), \n",
    "                'position_ids' : list(range(len(tokens)))\n",
    "            })\n",
    "\n",
    "        cache[file_path] = tuples \n",
    "\n",
    "    return cache \n",
    "\n",
    "def sweep(set: dict[str, list]) -> int:\n",
    "    max_tokens = 0\n",
    "    for k, v in set.items(): \n",
    "        for tuple in v: \n",
    "            max_tokens = max(max_tokens, len(tuple['input_ids']))\n",
    "    return max_tokens \n",
    "\n",
    "def peak(set: dict[str, list]) -> set[str]: \n",
    "    classes = {*()} # set() doesn't work  O_O\n",
    "    for k,v in set.items(): \n",
    "        for tuple in v: \n",
    "            classes.add(tuple['dialog_act'])\n",
    "    return classes  \n",
    "\n",
    "\n",
    "def pad(set: dict[str, list], max_tokens: int, pad_id: int): \n",
    "    for k, utterances in set.items(): \n",
    "        for row in utterances:\n",
    "            holdover = max_tokens - len(row['input_ids'])\n",
    "            row['input_ids'] = row['input_ids'] + ([pad_id] * holdover)\n",
    "            row['input_mask'] = row['input_mask'] + ([0] * holdover)\n",
    "            row['position_ids'] = row['position_ids'] + ([0] * holdover)\n",
    "\n",
    "            row['input_ids'] = mx.array(row['input_ids'])\n",
    "            row['input_mask'] = mx.array(row['input_mask'])\n",
    "            row['position_ids'] = mx.array(row['position_ids'])\n",
    "\n",
    "def convert_dialog(set: dict[str, list], dialogs: list): \n",
    "    for k, utterance in set.items(): \n",
    "        for row in utterance: \n",
    "            row['dialog_act'] = dialogs.index(row['dialog_act'])\n",
    "\n",
    "train_fp, test_fp, val_fp = map(glob, (f\"{data_dir}/train/*.txt\", f\"{data_dir}/test/*.txt\", f\"{data_dir}/val/*.txt\"))\n",
    "train, test, val = map(get_dialog, (train_fp, test_fp, val_fp))\n",
    "x, y, z = map(sweep, (train, test, val))\n",
    "\n",
    "# Useful for padding with maximum amount so that training doesn't require too much space. \n",
    "max_tokens = reduce(max, (x, y, z))\n",
    "list(map(lambda x: pad(x, max_tokens, tokenizer.pad_token_id), (train, test, val)))\n",
    "\n",
    "# Get dialog acts \n",
    "classes = list(reduce(lambda x, y: x | y, map(peak, (train, test, val))))\n",
    "num_classes = len(classes)\n",
    "\n",
    "list(map(lambda x: convert_dialog(x, classes), (train, test, val)))\n",
    "\n",
    "\n",
    "# Defined in paper as best chunk size \n",
    "chunk_size = 96 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ced44c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(model, X: tuple[list[str], mx.array, mx.array, mx.array], y : mx.array): \n",
    "    logits = model(X[1], X[2], X[3], X[0])\n",
    "\n",
    "    return mx.mean(nn.losses.cross_entropy(logits, y))\n",
    "\n",
    "def eval_fn(model, X: dict[str, list]) -> float: \n",
    "    pred_list = [] \n",
    "    y_list = [] \n",
    "\n",
    "    for k, v in X.items(): \n",
    "\n",
    "        for i in range(0, len(v), chunk_size):\n",
    "            slice = v[i : i + chunk_size]\n",
    "            \n",
    "            speakers = [row['speaker'] for row in slice]\n",
    "            acts = [row['dialog_act'] for row in slice]\n",
    "            input_ids = [row['input_ids'] for row in slice]\n",
    "            input_masks = [row['input_mask'] for row in slice]\n",
    "            position_ids = [row['position_ids'] for row in slice]\n",
    "\n",
    "            ids, masks, pos = map(\n",
    "                lambda x: mx.stack(x, axis = 0),\n",
    "                (input_ids, input_masks, position_ids) \n",
    "            )\n",
    "            acts = mx.array(acts)\n",
    "\n",
    "            logits = model(ids, masks, pos, speakers)\n",
    "            pred = mx.argmax(logits, axis = 1)\n",
    "\n",
    "            pred_list.append(pred)\n",
    "            y_list.append(acts)\n",
    "\n",
    "    pred_list = mx.concat(pred_list, axis = 0)\n",
    "    y_list = mx.concat(y_list, axis = 0)\n",
    "\n",
    "    acc = mx.sum((pred_list == y_list), axis = 0) / pred_list.shape[0]\n",
    "\n",
    "    return f1_score(np.array(y_list), np.array(pred_list), average = 'macro'), acc.item()\n",
    "\n",
    "from dagc.model import DAGC \n",
    "dagc = DAGC(model, num_classes)\n",
    "mx.eval(dagc.parameters())\n",
    "\n",
    "optimizer = optim.AdamW(learning_rate = 1e-3, weight_decay = 5e-4)\n",
    "\n",
    "loss_and_grad_fn = nn.value_and_grad(dagc, loss_fn)\n",
    "\n",
    "def step(speakers: list[str], \n",
    "         acts: list[int], \n",
    "         input_ids: list[mx.array], \n",
    "         input_masks: list[mx.array], \n",
    "         position_ids: list[mx.array]):\n",
    "    \n",
    "    ids, masks, pos = map(\n",
    "        lambda x: mx.stack(x, axis = 0),\n",
    "        (input_ids, input_masks, position_ids) \n",
    "    )\n",
    "    acts = mx.array(acts)\n",
    "\n",
    "    loss, grads = loss_and_grad_fn(dagc, (speakers, ids, masks, pos), acts)\n",
    "    optimizer.update(dagc, grads)\n",
    "\n",
    "    mx.eval(dagc.parameters(), optimizer.state)\n",
    "\n",
    "\n",
    "def early_stop(scores: list[float]) -> bool:\n",
    "    for i in range(2, 11):\n",
    "        if scores[-i] > scores[-(i - 1)]:\n",
    "            return False  \n",
    "    return True \n",
    "\n",
    "scores = [] \n",
    "lr_reduce_scores = [] \n",
    "for e in range(100):\n",
    "\n",
    "    for k, v in train.items(): \n",
    "\n",
    "        for i in range(0, len(v), chunk_size):\n",
    "            slice = v[i : i + chunk_size]\n",
    "            step(\n",
    "                [row['speaker'] for row in slice],\n",
    "                [row['dialog_act'] for row in slice],\n",
    "                [row['input_ids'] for row in slice],\n",
    "                [row['input_mask'] for row in slice],\n",
    "                [row['position_ids'] for row in slice]\n",
    "            )\n",
    "\n",
    "\n",
    "    score, acc = eval_fn(dagc, test)\n",
    "    scores.append(score)\n",
    "    lr_reduce_scores.append(score)\n",
    "    print(f\"Epoch {e + 1}, F1-Score: \", score, \"Accuracy: \", acc)\n",
    "\n",
    "    if len(scores) > 14:\n",
    "        if early_stop(scores):\n",
    "            print(\"Stopping early due to no f1 improvement\")\n",
    "            break  \n",
    "\n",
    "    if len(lr_reduce_scores) > 4: \n",
    "        if lr_reduce_scores[-1] <= lr_reduce_scores[-2] and lr_reduce_scores[-2] <= lr_reduce_scores[-3] and lr_reduce_scores[-3] <= lr_reduce_scores[-4]:\n",
    "            optimizer.learning_rate = optimizer.learning_rate * 0.9 \n",
    "            print(\"Reducing learning rate\")\n",
    "            lr_reduce_scores = [] \n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dagc-mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
